"""
AI Service - T√≠ch h·ª£p Google Gemini API
S·ª≠ d·ª•ng: google-generativeai library
Tu√¢n th·ªß: CHUCNANG.md Section 2.2 (Assessment), 2.6 (Chatbot)
"""

import json
from typing import List, Dict, Optional
import google.generativeai as genai
from config.config import get_settings
from models.models import Course, Lesson


# ============================================================================
# GEMINI API CONFIGURATION
# ============================================================================

settings = get_settings()
genai.configure(api_key=settings.google_api_key)
model = genai.GenerativeModel(settings.gemini_model)


# ============================================================================
# ASSESSMENT QUESTION GENERATION (Section 2.2.1-2.2.2)
# ============================================================================

async def generate_assessment_questions(
    category: str,
    subject: str,
    level: str,
    count: int = 10,
    focus_areas: Optional[List[str]] = None
) -> List[Dict]:
    """
    T·∫°o c√¢u h·ªèi ƒë√°nh gi√° nƒÉng l·ª±c s·ª≠ d·ª•ng Google Gemini
    Tu√¢n th·ªß CHUCNANG.md Section 2.2.1-2.2.2
    
    Ph√¢n b·ªï ƒë·ªô kh√≥ theo m·ª©c ƒë·ªô:
    - Beginner (15 c√¢u): 3 Easy (20%) + 8 Medium (53%) + 4 Hard (27%)
    - Intermediate (25 c√¢u): 5 Easy (20%) + 13 Medium (52%) + 7 Hard (28%)
    - Advanced (35 c√¢u): 7 Easy (20%) + 18 Medium (51%) + 10 Hard (29%)
    
    Args:
        category: Danh m·ª•c (Programming, Math, Business, Languages)
        subject: Ch·ªß ƒë·ªÅ c·ª• th·ªÉ (Python, Calculus, Marketing, etc.)
        level: M·ª©c ƒë·ªô (Beginner, Intermediate, Advanced)
        count: S·ªë l∆∞·ª£ng c√¢u h·ªèi (m·∫∑c ƒë·ªãnh 10, th·ª±c t·∫ø theo level)
        focus_areas: C√°c ch·ªß ƒë·ªÅ con c·∫ßn t·∫≠p trung (optional)
        
    Returns:
        List c√°c c√¢u h·ªèi v·ªõi c·∫•u tr√∫c ƒë·∫ßy ƒë·ªß theo CHUCNANG.md
    """
    # X√°c ƒë·ªãnh s·ªë c√¢u v√† ph√¢n b·ªï ƒë·ªô kh√≥ theo m·ª©c ƒë·ªô
    if level == "Beginner":
        total_questions = 15
        easy_count = 3
        medium_count = 8
        hard_count = 4
    elif level == "Intermediate":
        total_questions = 25
        easy_count = 5
        medium_count = 13
        hard_count = 7
    elif level == "Advanced":
        total_questions = 35
        easy_count = 7
        medium_count = 18
        hard_count = 10
    else:
        total_questions = count
        easy_count = int(count * 0.2)
        medium_count = int(count * 0.5)
        hard_count = count - easy_count - medium_count
    
    focus_areas_str = f"\n- T·∫≠p trung v√†o: {', '.join(focus_areas)}" if focus_areas else ""
    
    prompt = f"""
T·∫°o {total_questions} c√¢u h·ªèi ƒë√°nh gi√° nƒÉng l·ª±c cho:
- Danh m·ª•c: {category}
- Ch·ªß ƒë·ªÅ: {subject}
- M·ª©c ƒë·ªô: {level}{focus_areas_str}

Ph√¢n b·ªï ƒë·ªô kh√≥ CH√çNH X√ÅC:
- {easy_count} c√¢u EASY (20%): Ki·∫øn th·ª©c n·ªÅn t·∫£ng c∆° b·∫£n
- {medium_count} c√¢u MEDIUM (50-53%): √Åp d·ª•ng ki·∫øn th·ª©c th·ª±c t·∫ø
- {hard_count} c√¢u HARD (27-30%): T∆∞ duy ph·∫£n bi·ªán v√† gi·∫£i quy·∫øt v·∫•n ƒë·ªÅ ph·ª©c t·∫°p

Y√™u c·∫ßu format JSON ch√≠nh x√°c (s·∫Øp x·∫øp t·ª´ d·ªÖ ƒë·∫øn kh√≥):
[
    {{
        "question_id": "q1",
        "question_text": "C√¢u h·ªèi r√µ r√†ng, s√∫c t√≠ch v·ªõi ng·ªØ c·∫£nh th·ª±c t·∫ø...",
        "question_type": "multiple_choice",
        "options": ["Option A", "Option B", "Option C", "Option D"],
        "correct_answer_hint": "G·ª£i √Ω ng·∫Øn v·ªÅ ƒë√°p √°n ƒë√∫ng (kh√¥ng n√≥i tr·ª±c ti·∫øp)",
        "difficulty": "easy",
        "skill_tag": "python-syntax",
        "points": 1
    }},
    {{
        "question_id": "q2",
        "question_text": "C√¢u h·ªèi...",
        "question_type": "fill_in_blank",
        "options": null,
        "correct_answer_hint": "G·ª£i √Ω ng·∫Øn...",
        "difficulty": "medium",
        "skill_tag": "algorithm-basics",
        "points": 2
    }}
]

L∆∞u √Ω:
- S·∫Øp x·∫øp c√¢u h·ªèi t·ª´ D·ªÑ ‚Üí KH√ì
- question_type: multiple_choice, fill_in_blank, drag_and_drop
- difficulty: easy, medium, hard
- points: easy=1, medium=2, hard=3
- skill_tag: G·∫Øn nh√£n k·ªπ nƒÉng c·ª• th·ªÉ (v√≠ d·ª•: python-syntax, data-structures-array)
- options: null n·∫øu kh√¥ng ph·∫£i multiple_choice
- PH·∫¢I tr·∫£ v·ªÅ JSON array h·ª£p l·ªá, kh√¥ng th√™m text kh√°c
"""
    
    try:
        response = model.generate_content(prompt)
        response_text = response.text.strip()
        
        # Lo·∫°i b·ªè markdown code blocks n·∫øu c√≥
        if response_text.startswith("```json"):
            response_text = response_text[7:]
        if response_text.startswith("```"):
            response_text = response_text[3:]
        if response_text.endswith("```"):
            response_text = response_text[:-3]
        
        response_text = response_text.strip()
        
        # Parse JSON
        questions = json.loads(response_text)
        
        return questions
    
    except json.JSONDecodeError as e:
        # Fallback: t·∫°o c√¢u h·ªèi m·∫´u n·∫øu parse JSON th·∫•t b·∫°i
        print(f"JSON parse error: {str(e)}")
        if level == "Beginner":
            return _generate_fallback_questions(category, subject, level, 15)
        elif level == "Intermediate":
            return _generate_fallback_questions(category, subject, level, 25)
        elif level == "Advanced":
            return _generate_fallback_questions(category, subject, level, 35)
        return _generate_fallback_questions(category, subject, level, count)
    
    except Exception as e:
        # Log error v√† tr·∫£ v·ªÅ fallback
        print(f"Error generating questions: {str(e)}")
        if level == "Beginner":
            return _generate_fallback_questions(category, subject, level, 15)
        elif level == "Intermediate":
            return _generate_fallback_questions(category, subject, level, 25)
        elif level == "Advanced":
            return _generate_fallback_questions(category, subject, level, 35)
        return _generate_fallback_questions(category, subject, level, count)


def _generate_fallback_questions(
    category: str,
    subject: str,
    level: str,
    count: int
) -> List[Dict]:
    """
    T·∫°o c√¢u h·ªèi fallback khi Gemini API th·∫•t b·∫°i
    Tu√¢n th·ªß ph√¢n b·ªï ƒë·ªô kh√≥ theo CHUCNANG.md
    
    Returns:
        List c√¢u h·ªèi m·∫´u v·ªõi ph√¢n b·ªï ƒë√∫ng easy/medium/hard
    """
    # Ph√¢n b·ªï ƒë·ªô kh√≥
    if count == 15:  # Beginner
        easy_count, medium_count, hard_count = 3, 8, 4
    elif count == 25:  # Intermediate
        easy_count, medium_count, hard_count = 5, 13, 7
    elif count == 35:  # Advanced
        easy_count, medium_count, hard_count = 7, 18, 10
    else:
        easy_count = int(count * 0.2)
        medium_count = int(count * 0.5)
        hard_count = count - easy_count - medium_count
    
    questions = []
    question_num = 1
    
    # T·∫°o c√¢u d·ªÖ
    for i in range(easy_count):
        questions.append({
            "question_id": f"q{question_num}",
            "question_text": f"[Fallback] C√¢u h·ªèi c∆° b·∫£n v·ªÅ {subject} - c√¢u {question_num}",
            "question_type": "multiple_choice",
            "options": ["Option A", "Option B", "Option C", "Option D"],
            "correct_answer_hint": f"G·ª£i √Ω: Xem x√©t ki·∫øn th·ª©c c∆° b·∫£n v·ªÅ {subject}",
            "difficulty": "easy",
            "skill_tag": f"{subject.lower()}-basics",
            "points": 1
        })
        question_num += 1
    
    # T·∫°o c√¢u trung b√¨nh
    for i in range(medium_count):
        questions.append({
            "question_id": f"q{question_num}",
            "question_text": f"[Fallback] C√¢u h·ªèi √°p d·ª•ng v·ªÅ {subject} - c√¢u {question_num}",
            "question_type": "multiple_choice",
            "options": ["Option A", "Option B", "Option C", "Option D"],
            "correct_answer_hint": f"G·ª£i √Ω: √Åp d·ª•ng ki·∫øn th·ª©c {subject} v√†o th·ª±c t·∫ø",
            "difficulty": "medium",
            "skill_tag": f"{subject.lower()}-application",
            "points": 2
        })
        question_num += 1
    
    # T·∫°o c√¢u kh√≥
    for i in range(hard_count):
        questions.append({
            "question_id": f"q{question_num}",
            "question_text": f"[Fallback] C√¢u h·ªèi ph·ª©c t·∫°p v·ªÅ {subject} - c√¢u {question_num}",
            "question_type": "multiple_choice",
            "options": ["Option A", "Option B", "Option C", "Option D"],
            "correct_answer_hint": f"G·ª£i √Ω: C·∫ßn t∆∞ duy ph·∫£n bi·ªán v·ªÅ {subject}",
            "difficulty": "hard",
            "skill_tag": f"{subject.lower()}-advanced",
            "points": 3
        })
        question_num += 1
    
    return questions


# ============================================================================
# ASSESSMENT EVALUATION (Section 2.2.3)
# ============================================================================

async def evaluate_assessment_answers(
    questions: List[Dict],
    answers: List[Dict],
    category: str,
    subject: str
) -> Dict:
    """
    ƒê√°nh gi√° k·∫øt qu·∫£ assessment v√† ph√¢n t√≠ch nƒÉng l·ª±c
    Tu√¢n th·ªß CHUCNANG.md Section 2.2.3
    
    T√≠nh ƒëi·ªÉm c√≥ tr·ªçng s·ªë: easy=1, medium=2, hard=3
    Ph√¢n t√≠ch theo 4 kh√≠a c·∫°nh:
    1. ƒêi·ªÉm t·ªïng th·ªÉ (0-100)
    2. Ph√¢n lo·∫°i tr√¨nh ƒë·ªô (Beginner < 60, Intermediate 60-80, Advanced > 80)
    3. ƒêi·ªÉm m·∫°nh/y·∫øu theo skill tag
    4. L·ªó h·ªïng ki·∫øn th·ª©c c·∫ßn kh·∫Øc ph·ª•c
    
    Args:
        questions: List c√¢u h·ªèi
        answers: List c√¢u tr·∫£ l·ªùi c·ªßa user
        category: Danh m·ª•c
        subject: Ch·ªß ƒë·ªÅ
        
    Returns:
        Dict ch·ª©a k·∫øt qu·∫£ ph√¢n t√≠ch ƒë·∫ßy ƒë·ªß
    """
    # T√≠nh ƒëi·ªÉm c√≥ tr·ªçng s·ªë
    total_weighted_score = 0
    max_weighted_score = 0
    correct_count = 0
    total_count = len(questions)
    
    # Ph√¢n t√≠ch theo ƒë·ªô kh√≥
    easy_correct = easy_total = 0
    medium_correct = medium_total = 0
    hard_correct = hard_total = 0
    
    # Ph√¢n t√≠ch theo skill tag
    skill_stats = {}
    
    answer_map = {ans.get("question_id"): ans.get("answer_content", ans.get("answer", "")) for ans in answers}
    
    # S·ª≠ d·ª•ng AI ƒë·ªÉ ƒë√°nh gi√° t·ª´ng c√¢u (v√¨ ch·ªâ c√≥ correct_answer_hint, kh√¥ng c√≥ ƒë√°p √°n th·ª±c)
    evaluation_pairs = []
    for question in questions:
        q_id = question["question_id"]
        user_answer = answer_map.get(q_id, "")
        evaluation_pairs.append({
            "question_id": q_id,
            "question_text": question.get("question_text", ""),
            "question_type": question.get("question_type", ""),
            "options": question.get("options"),
            "correct_answer_hint": question.get("correct_answer_hint", ""),
            "user_answer": user_answer,
            "difficulty": question.get("difficulty", "medium"),
            "skill_tag": question.get("skill_tag", "general"),
            "points": question.get("points", 2)
        })
    
    # IMPORTANT FIX: Batch evaluation ƒë·ªÉ tr√°nh timeout v·ªõi nhi·ªÅu c√¢u h·ªèi
    grading_map = {}
    batch_size = 10  # Evaluate 10 questions at a time
    
    for batch_start in range(0, len(evaluation_pairs), batch_size):
        batch_end = min(batch_start + batch_size, len(evaluation_pairs))
        batch_pairs = evaluation_pairs[batch_start:batch_end]
        
        grading_prompt = f"""
Ch·∫•m ƒëi·ªÉm c√°c c√¢u tr·∫£ l·ªùi sau (tr·∫£ v·ªÅ JSON array):

{json.dumps(batch_pairs, ensure_ascii=False, indent=2)}

H√£y tr·∫£ v·ªÅ JSON array v·ªõi format:
[
    {{
        "question_id": "q1",
        "is_correct": true,
        "explanation": "ƒê√°p √°n ƒë√∫ng v√¨..."
    }},
    ...
]

L∆∞u √Ω:
- is_correct: true/false d·ª±a tr√™n so s√°nh user_answer v·ªõi correct_answer_hint
- V·ªõi multiple_choice: ki·ªÉm tra user_answer c√≥ kh·ªõp v·ªõi ƒë√°p √°n g·ª£i √Ω
- PH·∫¢I tr·∫£ v·ªÅ JSON array h·ª£p l·ªá
"""
        
        try:
            grading_response = model.generate_content(grading_prompt)
            grading_text = grading_response.text.strip()
            
            # Clean markdown
            if grading_text.startswith("```json"):
                grading_text = grading_text[7:]
            if grading_text.startswith("```"):
                grading_text = grading_text[3:]
            if grading_text.endswith("```"):
                grading_text = grading_text[:-3]
            grading_text = grading_text.strip()
            
            batch_results = json.loads(grading_text)
            for r in batch_results:
                grading_map[r["question_id"]] = r["is_correct"]
        
        except Exception as e:
            print(f"AI grading error for batch {batch_start}-{batch_end}: {e}. Using fallback.")
            # Fallback: ch·∫•m ƒë∆°n gi·∫£n b·∫±ng keyword matching v·ªõi hint
            for pair in batch_pairs:
                hint = pair["correct_answer_hint"].lower()
                answer = pair["user_answer"].lower()
                # Simple heuristic: n·∫øu answer ch·ª©a t·ª´ kh√≥a trong hint
                grading_map[pair["question_id"]] = (hint in answer or answer in hint)
    
    # T√≠nh ƒëi·ªÉm d·ª±a tr√™n k·∫øt qu·∫£ AI grading
    for question in questions:
        q_id = question["question_id"]
        difficulty = question.get("difficulty", "medium").lower()
        skill_tag = question.get("skill_tag", "general")
        points = question.get("points", 2)
        
        # C·∫≠p nh·∫≠t max score
        max_weighted_score += points
        
        # Ki·ªÉm tra ƒë√°p √°n t·ª´ AI
        is_correct = grading_map.get(q_id, False)
        if is_correct:
            correct_count += 1
            total_weighted_score += points
        
        # Th·ªëng k√™ theo ƒë·ªô kh√≥
        if difficulty == "easy":
            easy_total += 1
            if is_correct:
                easy_correct += 1
        elif difficulty == "hard":
            hard_total += 1
            if is_correct:
                hard_correct += 1
        else:  # medium
            medium_total += 1
            if is_correct:
                medium_correct += 1
        
        # Th·ªëng k√™ theo skill tag
        if skill_tag not in skill_stats:
            skill_stats[skill_tag] = {"total": 0, "correct": 0}
        skill_stats[skill_tag]["total"] += 1
        if is_correct:
            skill_stats[skill_tag]["correct"] += 1
    
    # T√≠nh ƒëi·ªÉm t·ªïng th·ªÉ (0-100)
    overall_score = (total_weighted_score / max_weighted_score * 100) if max_weighted_score > 0 else 0
    
    # X√°c ƒë·ªãnh proficiency level theo ng∆∞·ª°ng
    if overall_score >= 80:
        proficiency_level = "Advanced"
    elif overall_score >= 60:
        proficiency_level = "Intermediate"
    else:
        proficiency_level = "Beginner"
    
    # Chu·∫©n b·ªã d·ªØ li·ªáu cho Gemini ph√¢n t√≠ch
    skill_summary = []
    for skill, stats in skill_stats.items():
        percentage = (stats["correct"] / stats["total"] * 100) if stats["total"] > 0 else 0
        skill_summary.append({
            "skill": skill,
            "correct": stats["correct"],
            "total": stats["total"],
            "percentage": percentage
        })
    
    # S·ª≠ d·ª•ng Gemini ƒë·ªÉ ph√¢n t√≠ch chi ti·∫øt
    analysis_prompt = f"""
Ph√¢n t√≠ch k·∫øt qu·∫£ assessment chi ti·∫øt:
- Danh m·ª•c: {category}
- Ch·ªß ƒë·ªÅ: {subject}
- ƒêi·ªÉm s·ªë: {overall_score:.1f}/100 (c√≥ tr·ªçng s·ªë)
- S·ªë c√¢u ƒë√∫ng: {correct_count}/{total_count}
- Ph√¢n lo·∫°i: {proficiency_level}

Ph√¢n t√≠ch theo ƒë·ªô kh√≥:
- Easy: {easy_correct}/{easy_total} ƒë√∫ng ({easy_correct/easy_total*100 if easy_total > 0 else 0:.1f}%)
- Medium: {medium_correct}/{medium_total} ƒë√∫ng ({medium_correct/medium_total*100 if medium_total > 0 else 0:.1f}%)
- Hard: {hard_correct}/{hard_total} ƒë√∫ng ({hard_correct/hard_total*100 if hard_total > 0 else 0:.1f}%)

Ph√¢n t√≠ch theo skill tag:
{json.dumps(skill_summary, indent=2, ensure_ascii=False)}

H√£y tr·∫£ v·ªÅ JSON v·ªõi format (theo CHUCNANG.md Section 2.2.3):
{{
    "skill_analysis": [
        {{
            "skill_tag": "python-syntax",
            "questions_count": 5,
            "correct_count": 4,
            "proficiency_percentage": 80.0,
            "strength_level": "Strong",
            "detailed_feedback": "B·∫°n n·∫Øm v·ªØng c√∫ ph√°p Python c∆° b·∫£n..."
        }}
    ],
    "knowledge_gaps": [
        {{
            "gap_area": "Algorithm Complexity",
            "description": "Ch∆∞a hi·ªÉu r√µ v·ªÅ ƒë·ªô ph·ª©c t·∫°p thu·∫≠t to√°n O(n), O(log n)",
            "importance": "High",
            "suggested_action": "H·ªçc l·∫°i Big O notation v√† ph√¢n t√≠ch thu·∫≠t to√°n"
        }}
    ],
    "overall_feedback": "Nh·∫≠n x√©t t·ªïng quan chi ti·∫øt v·ªÅ nƒÉng l·ª±c..."
}}

L∆∞u √Ω:
- strength_level: Strong (>80%), Average (60-80%), Weak (<60%)
- importance: High, Medium, Low
- PH·∫¢I tr·∫£ v·ªÅ JSON h·ª£p l·ªá, kh√¥ng th√™m text kh√°c
"""
    
    try:
        response = model.generate_content(analysis_prompt)
        response_text = response.text.strip()
        
        # Lo·∫°i b·ªè markdown
        if response_text.startswith("```json"):
            response_text = response_text[7:]
        if response_text.startswith("```"):
            response_text = response_text[3:]
        if response_text.endswith("```"):
            response_text = response_text[:-3]
        
        response_text = response_text.strip()
        
        analysis = json.loads(response_text)
        
        # Return structure matching what assessment_service expects
        return {
            "overall_score": round(overall_score, 1),
            "proficiency_level": proficiency_level,
            "skill_analysis": analysis.get("skill_analysis", []),  # List[SkillAnalysis]
            "knowledge_gaps": analysis.get("knowledge_gaps", []),   # List[KnowledgeGap]
            "overall_feedback": analysis.get("overall_feedback", ""),  # String
            "score_breakdown": {
                "easy": {"correct": easy_correct, "total": easy_total},
                "medium": {"correct": medium_correct, "total": medium_total},
                "hard": {"correct": hard_correct, "total": hard_total}
            }
        }
    
    except Exception as e:
        print(f"Error in AI analysis: {str(e)}")
        # Fallback analysis v·ªõi logic ƒë∆°n gi·∫£n
        skill_analysis_list = []
        knowledge_gaps_list = []
        
        for skill, stats in skill_stats.items():
            percentage = (stats["correct"] / stats["total"] * 100) if stats["total"] > 0 else 0
            
            if percentage >= 80:
                strength_level = "Strong"
            elif percentage >= 60:
                strength_level = "Average"
            else:
                strength_level = "Weak"
                # N·∫øu y·∫øu, th√™m v√†o knowledge gaps
                knowledge_gaps_list.append({
                    "gap_area": skill,
                    "description": f"Ch·ªâ ƒë·∫°t {percentage:.1f}% ·ªü k·ªπ nƒÉng {skill}",
                    "importance": "High" if percentage < 40 else "Medium",
                    "suggested_action": f"C·∫ßn √¥n luy·ªán th√™m v·ªÅ {skill}"
                })
            
            skill_analysis_list.append({
                "skill_tag": skill,
                "questions_count": stats["total"],
                "correct_count": stats["correct"],
                "proficiency_percentage": round(percentage, 1),
                "strength_level": strength_level,
                "detailed_feedback": f"B·∫°n ƒë·∫°t {percentage:.1f}% ·ªü k·ªπ nƒÉng {skill}"
            })
        
        # Fallback return structure (same format as AI analysis)
        return {
            "overall_score": round(overall_score, 1),
            "proficiency_level": proficiency_level,
            "skill_analysis": skill_analysis_list,
            "knowledge_gaps": knowledge_gaps_list,
            "overall_feedback": f"B·∫°n ƒë·∫°t {overall_score:.1f}% - m·ª©c {proficiency_level}. T·ªïng {correct_count}/{total_count} c√¢u ƒë√∫ng.",
            "score_breakdown": {
                "easy": {"correct": easy_correct, "total": easy_total},
                "medium": {"correct": medium_correct, "total": medium_total},
                "hard": {"correct": hard_correct, "total": hard_total}
            }
        }


# ============================================================================
# CHATBOT v·ªõi Course Context (Section 2.6)
# ============================================================================

async def chat_with_course_context(
    course_id: str,
    question: str,
    conversation_history: Optional[List[Dict]] = None
) -> str:
    """
    Chatbot tr·∫£ l·ªùi c√¢u h·ªèi d·ª±a tr√™n context c·ªßa kh√≥a h·ªçc
    
    Args:
        course_id: ID c·ªßa kh√≥a h·ªçc
        question: C√¢u h·ªèi t·ª´ user
        conversation_history: L·ªãch s·ª≠ chat tr∆∞·ªõc ƒë√≥ (optional)
        
    Returns:
        C√¢u tr·∫£ l·ªùi t·ª´ AI
    """
    import logging
    logger = logging.getLogger(__name__)
    
    try:
        # L·∫•y th√¥ng tin kh√≥a h·ªçc t·ª´ database
        course = await Course.get(course_id)
        
        if not course:
            logger.warning(f"Course not found: {course_id}")
            return "Kh√¥ng t√¨m th·∫•y th√¥ng tin kh√≥a h·ªçc."
        
        # T·∫°o context t·ª´ kh√≥a h·ªçc (t√≥m g·ªçn)
        course_context = _build_course_context(course)
        
        # T·∫°o conversation context (5 messages g·∫ßn nh·∫•t - ƒë·ªß cho AI free)
        history_text = ""
        if conversation_history and len(conversation_history) > 0:
            for msg in conversation_history[-5:]:
                role = msg.get("role", "user")
                content = msg.get("content", "")
                history_text += f"{role}: {content}\n"
        
        # T·∫°o prompt
        prompt = f"""
B·∫°n l√† tr·ª£ l√Ω AI h·ªó tr·ª£ h·ªçc t·∫≠p cho kh√≥a h·ªçc sau:

TH√îNG TIN KH√ìA H·ªåC:
{course_context}

L·ªäCH S·ª¨ H·ªòI THO·∫†I:
{history_text}

C√ÇU H·ªéI M·ªöI T·ª™ H·ªåC VI√äN:
{question}

H√£y tr·∫£ l·ªùi c√¢u h·ªèi d·ª±a tr√™n th√¥ng tin kh√≥a h·ªçc. Tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát, ng·∫Øn g·ªçn v√† d·ªÖ hi·ªÉu.
N·∫øu c√¢u h·ªèi kh√¥ng li√™n quan ƒë·∫øn kh√≥a h·ªçc, h√£y l·ªãch s·ª± nh·∫Øc nh·ªü h·ªçc vi√™n t·∫≠p trung v√†o n·ªôi dung kh√≥a h·ªçc.
"""
        
        response = model.generate_content(prompt)
        return response.text.strip()
    
    except Exception as e:
        logger.error(f"Error in chat_with_course_context: {str(e)}", exc_info=True)
        return "Xin l·ªói, t√¥i kh√¥ng th·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi n√†y l√∫c n√†y. Vui l√≤ng th·ª≠ l·∫°i sau."


def _build_course_context(course: Course) -> str:
    """
    T·∫°o text context t·ª´ Course document (t√≥m g·ªçn cho AI free)
    Ch·ªâ l·∫•y th√¥ng tin c∆° b·∫£n: title, description, modules/lessons structure
    
    Args:
        course: Course document
        
    Returns:
        String ch·ª©a th√¥ng tin kh√≥a h·ªçc (t·ªëi ∆∞u cho token limit)
    """
    context = f"""
T√™n kh√≥a h·ªçc: {course.title}
M√¥ t·∫£: {course.description}
Danh m·ª•c: {course.category}
M·ª©c ƒë·ªô: {course.level}

K·∫øt qu·∫£ h·ªçc t·∫≠p:
"""
    
    # Ch·ªâ l·∫•y t·ªëi ƒëa 5 learning outcomes ƒë·∫ßu ti√™n
    for i, outcome in enumerate(course.learning_outcomes[:5]):
        context += f"- {outcome.get('description', '')}\n"
        if i >= 4:  # Gi·ªõi h·∫°n 5 outcomes
            break
    
    context += "\nN·ªôi dung kh√≥a h·ªçc:\n"
    
    # Ch·ªâ li·ªát k√™ c·∫•u tr√∫c modules v√† lessons (kh√¥ng th√™m description chi ti·∫øt)
    for idx, module in enumerate(course.modules[:10]):  # Gi·ªõi h·∫°n 10 modules
        context += f"\n## Module {idx + 1}: {module.title}\n"
        
        # Ch·ªâ list t√™n lessons, kh√¥ng th√™m description
        for lesson in module.lessons[:8]:  # Gi·ªõi h·∫°n 8 lessons/module
            context += f"  - B√†i {lesson.order}: {lesson.title}\n"
    
    return context


# ============================================================================
# COURSE RECOMMENDATION (Section 2.7.4)
# ============================================================================

async def generate_course_recommendations(
    user_learning_history: List[Dict],
    assessment_results: Optional[Dict] = None,
    available_courses: Optional[List[Course]] = None
) -> Dict:
    """
    T·∫°o ƒë·ªÅ xu·∫•t kh√≥a h·ªçc d·ª±a tr√™n l·ªãch s·ª≠ h·ªçc t·∫≠p v√† k·∫øt qu·∫£ assessment
    
    Args:
        user_learning_history: L·ªãch s·ª≠ h·ªçc t·∫≠p c·ªßa user
        assessment_results: K·∫øt qu·∫£ assessment (optional)
        available_courses: Danh s√°ch kh√≥a h·ªçc c√≥ s·∫µn (optional)
        
    Returns:
        Dict ch·ª©a:
        {
            "recommended_courses": [
                {
                    "course_id": "...",
                    "title": "...",
                    "reason": "...",
                    "priority": 1
                }
            ],
            "learning_path": [...],
            "ai_advice": "..."
        }
    """
    # Build available courses context
    courses_context = "Kh√¥ng c√≥ th√¥ng tin kh√≥a h·ªçc"
    if available_courses:
        courses_list = []
        for idx, course in enumerate(available_courses[:15], 1):  # Limit to 15 courses to avoid token overflow
            outcomes = ", ".join([o.get('description', '')[:50] for o in course.learning_outcomes[:3]])
            courses_list.append(
                f"{idx}. [{course.id}] {course.title}\n"
                f"   - Level: {course.level} | Category: {course.category}\n"
                f"   - M√¥ t·∫£: {course.description[:150]}...\n"
                f"   - H·ªçc ƒë∆∞·ª£c: {outcomes}"
            )
        courses_context = "\n\n".join(courses_list)
    
    # Extract knowledge gaps for better context
    knowledge_gaps_str = ""
    if assessment_results and assessment_results.get("knowledge_gaps"):
        gaps = [f"- {gap.get('gap_area', gap.get('topic', 'Unknown'))}: {gap.get('description', '')}" 
                for gap in assessment_results["knowledge_gaps"][:5]]
        knowledge_gaps_str = "\n".join(gaps)
    
    # T·∫°o prompt cho Gemini v·ªõi available courses
    prompt = f"""
D·ª±a tr√™n th√¥ng tin h·ªçc vi√™n:

L·ªäCH S·ª¨ H·ªåC T·∫¨P:
{json.dumps(user_learning_history, indent=2, ensure_ascii=False)}

K·∫æT QU·∫¢ ASSESSMENT:
{json.dumps(assessment_results, indent=2, ensure_ascii=False) if assessment_results else "Ch∆∞a c√≥"}

L·ªñ H·ªîNG KI·∫æN TH·ª®C C·∫¶N KH·∫ÆC PH·ª§C:
{knowledge_gaps_str if knowledge_gaps_str else "Ch∆∞a x√°c ƒë·ªãnh"}

C√ÅC KH√ìA H·ªåC C√ì S·∫¥N TRONG H·ªÜ TH·ªêNG:
{courses_context}

Nhi·ªám v·ª•: D·ª±a tr√™n l·ªó h·ªïng ki·∫øn th·ª©c v√† kh√≥a h·ªçc c√≥ s·∫µn, h√£y ƒë·ªÅ xu·∫•t:
1. C√°c course_id C·ª§ TH·ªÇ t·ª´ danh s√°ch tr√™n (ch·ªçn 3-5 kh√≥a ph√π h·ª£p nh·∫•t)
2. L·ªô tr√¨nh h·ªçc (th·ª© t·ª± n√™n h·ªçc c√°c kh√≥a)
3. L·ªùi khuy√™n c√° nh√¢n h√≥a

Tr·∫£ v·ªÅ JSON format (QUAN TR·ªåNG - ch·ªâ d√πng course_id t·ª´ danh s√°ch tr√™n):
{{
    "recommended_course_ids": [
        {{
            "course_id": "abc-123-xyz",
            "reason": "Kh√≥a n√†y gi√∫p kh·∫Øc ph·ª•c l·ªó h·ªïng v·ªÅ...",
            "priority": 1
        }}
    ],
    "learning_sequence": [
        {{
            "course_id": "abc-123-xyz",
            "focus_modules": ["Module v·ªÅ X", "Module v·ªÅ Y"],
            "why_this_order": "H·ªçc tr∆∞·ªõc v√¨ l√† n·ªÅn t·∫£ng"
        }}
    ],
    "ai_advice": "L·ªùi khuy√™n chi ti·∫øt cho h·ªçc vi√™n..."
}}

L∆∞u √Ω: course_id PH·∫¢I l·∫•y t·ª´ danh s√°ch [id] ·ªü tr√™n, KH√îNG t·ª± nghƒ© ra.
PH·∫¢I tr·∫£ v·ªÅ JSON h·ª£p l·ªá.
"""
    
    try:
        response = model.generate_content(prompt)
        response_text = response.text.strip()
        
        # Lo·∫°i b·ªè markdown
        if response_text.startswith("```json"):
            response_text = response_text[7:]
        if response_text.startswith("```"):
            response_text = response_text[3:]
        if response_text.endswith("```"):
            response_text = response_text[:-3]
        
        response_text = response_text.strip()
        
        recommendations = json.loads(response_text)
        
        # Validate and extract recommended courses
        recommended_course_ids = recommendations.get("recommended_course_ids", [])
        
        return {
            "recommended_courses": recommended_course_ids,  # Now contains course_id + reason
            "learning_path": recommendations.get("learning_sequence", []),
            "ai_advice": recommendations.get("ai_advice", "")
        }
    
    except Exception as e:
        print(f"Error in generate_course_recommendations: {str(e)}")
        return {
            "recommended_courses": [],
            "learning_path": [],
            "ai_advice": "H√£y ti·∫øp t·ª•c h·ªçc t·∫≠p v√† ho√†n th√†nh c√°c kh√≥a h·ªçc hi·ªán t·∫°i."
        }


# ============================================================================
# PERSONAL COURSE GENERATION
# ============================================================================

async def generate_course_from_prompt(
    prompt: str,
    user_preferences: Optional[List[str]] = None,
    difficulty: str = "Beginner"
) -> Dict[str, any]:
    """
    Generate a complete course structure from a text prompt using AI.
    
    Args:
        prompt: User's description of desired course
        user_preferences: User's learning preferences
        difficulty: Course difficulty level
        
    Returns:
        Dict containing course structure (title, description, modules, lessons)
    """
    try:
        # Build prompt for AI
        system_prompt = f"""B·∫°n l√† m·ªôt chuy√™n gia thi·∫øt k·∫ø kh√≥a h·ªçc. 
Nhi·ªám v·ª•: T·∫°o c·∫•u tr√∫c kh√≥a h·ªçc ho√†n ch·ªânh d·ª±a tr√™n y√™u c·∫ßu c·ªßa ng∆∞·ªùi d√πng.
C·∫•p ƒë·ªô: {difficulty}
S·ªü th√≠ch ng∆∞·ªùi d√πng: {', '.join(user_preferences) if user_preferences else 'Kh√¥ng c√≥'}

Tr·∫£ v·ªÅ JSON v·ªõi c·∫•u tr√∫c (CH·ªà JSON THU·∫¶N T√öY, KH√îNG C√ì TEXT KH√ÅC):
{{
    "title": "T√™n kh√≥a h·ªçc ng·∫Øn g·ªçn",
    "description": "M√¥ t·∫£ ng·∫Øn (max 200 chars)",
    "category": "Programming|Data Science|Business",
    "level": "{difficulty}",
    "estimated_duration": 30,
    "modules": [
        {{
            "title": "T√™n module",
            "description": "M√¥ t·∫£ ng·∫Øn",
            "order": 1,
            "difficulty": "Basic|Intermediate|Advanced",
            "estimated_hours": 2,
            "learning_outcomes": [
                {{
                    "description": "M·ª•c ti√™u ng·∫Øn g·ªçn",
                    "skill_tag": "tag-k·ªπ-nƒÉng"
                }}
            ],
            "lessons": [
                {{
                    "title": "T√™n b√†i",
                    "description": "M√¥ t·∫£ ng·∫Øn",
                    "content": "N·ªôi dung ng·∫Øn g·ªçn",
                    "duration_minutes": 15,
                    "order": 1
                }}
            ]
        }}
    ]
}}

L∆ØU √ù:
- CH·ªà tr·∫£ v·ªÅ JSON, KH√îNG c√≥ markdown hay text th·ª´a
- M√¥ t·∫£ ng·∫Øn g·ªçn, tr√°nh qu√° d√†i
- M·ªói module 2-3 learning outcomes
- M·ªói module 2-3 lessons
- Max 3-4 modules
"""

        user_prompt = f"T·∫°o kh√≥a h·ªçc: {prompt[:200]}"  # Gi·ªõi h·∫°n prompt
        
        # Generate course structure
        print(f"ü§ñ Calling Gemini API for course generation...", flush=True)
        response = model.generate_content(
            [system_prompt, user_prompt],
            generation_config=genai.types.GenerationConfig(
                temperature=0.7,
                max_output_tokens=8000  # TƒÉng l√™n 8000
            )
        )
        
        print(f"‚úÖ Gemini API responded", flush=True)
        
        # Parse response
        response_text = response.text.strip()
        print(f"üìÑ Response length: {len(response_text)} characters", flush=True)
        
        # Extract JSON - IMPROVED LOGIC
        json_str = None
        
        # Method 1: Check for ```json specifically
        if "```json" in response_text:
            print(f"üìã Found ```json block", flush=True)
            json_start = response_text.find("```json") + 7
            json_end = response_text.find("```", json_start)
            if json_end > json_start:
                json_str = response_text[json_start:json_end].strip()
                print(f"üìã Method 1 extracted {len(json_str)} chars", flush=True)
            else:
                print(f"üìã Method 1 failed: json_end ({json_end}) <= json_start ({json_start})", flush=True)
        
        # Method 2: Look for JSON object directly (starts with {)
        if not json_str:
            print(f"üìã Method 1 didn't work, trying Method 2...", flush=True)
            # Find first { and last }
            first_brace = response_text.find("{")
            last_brace = response_text.rfind("}")
            
            if first_brace != -1 and last_brace != -1 and last_brace > first_brace:
                print(f"üìã Found JSON object at position {first_brace}-{last_brace}", flush=True)
                json_str = response_text[first_brace:last_brace+1]
            else:
                print(f"‚ùå No JSON object found in response", flush=True)
                raise ValueError("No valid JSON found in AI response")
        else:
            print(f"üìã Using Method 1 result", flush=True)
        
        print(f"üìã Extracted JSON length: {len(json_str)} chars", flush=True)
        print(f"üìã JSON preview: {json_str[:300]}...", flush=True)
        
        course_data = json.loads(json_str)
        
        # IMPORTANT FIX: Validate required fields ƒë·ªÉ tr√°nh crash
        required_fields = ["title", "description", "category", "level", "modules"]
        for field in required_fields:
            if field not in course_data:
                raise ValueError(f"AI response missing required field: {field}")
        
        # Validate modules structure
        if not isinstance(course_data.get("modules"), list) or len(course_data["modules"]) == 0:
            raise ValueError("AI response has invalid or empty modules")
        
        # Validate each module has required fields
        for idx, module in enumerate(course_data["modules"]):
            if not isinstance(module, dict):
                raise ValueError(f"Module {idx} is not a dict")
            if "title" not in module or "lessons" not in module:
                raise ValueError(f"Module {idx} missing title or lessons")
            if not isinstance(module["lessons"], list) or len(module["lessons"]) == 0:
                raise ValueError(f"Module {idx} has invalid or empty lessons")
        
        return course_data
        
    except (json.JSONDecodeError, ValueError, KeyError) as e:
        print(f"‚ùå AI course generation error: {type(e).__name__}: {str(e)}", flush=True)
        print(f"üìù JSON preview: {json_str[:500] if 'json_str' in locals() else 'N/A'}", flush=True)
        print(f"üîÑ Returning fallback structure...", flush=True)
        # Fallback structure v·ªõi learning_outcomes ƒë√∫ng format
        return {
            "title": f"Kh√≥a h·ªçc v·ªÅ {prompt[:50]}",
            "description": f"Kh√≥a h·ªçc ƒë∆∞·ª£c t·∫°o t·ª± ƒë·ªông d·ª±a tr√™n y√™u c·∫ßu: {prompt}",
            "category": "General",
            "level": difficulty,
            "learning_outcomes": [
                {
                    "id": "lo1",
                    "description": f"Hi·ªÉu ƒë∆∞·ª£c kh√°i ni·ªám c∆° b·∫£n v·ªÅ {prompt[:30]}",
                    "skill_tag": "basic-understanding"
                },
                {
                    "id": "lo2",
                    "description": "√Åp d·ª•ng ƒë∆∞·ª£c ki·∫øn th·ª©c v√†o th·ª±c t·∫ø",
                    "skill_tag": "practical-application"
                }
            ],
            "modules": [
                {
                    "title": "Module 1: Gi·ªõi thi·ªáu",
                    "description": "Module gi·ªõi thi·ªáu kh√≥a h·ªçc",
                    "order": 1,
                    "difficulty": "Basic",
                    "estimated_hours": 2,
                    "learning_outcomes": [
                        {
                            "description": "N·∫Øm ƒë∆∞·ª£c t·ªïng quan v·ªÅ kh√≥a h·ªçc",
                            "skill_tag": "overview"
                        },
                        {
                            "description": "Chu·∫©n b·ªã ki·∫øn th·ª©c n·ªÅn t·∫£ng",
                            "skill_tag": "foundation"
                        }
                    ],
                    "lessons": [
                        {
                            "title": "B√†i 1: T·ªïng quan",
                            "description": "Gi·ªõi thi·ªáu t·ªïng quan v·ªÅ kh√≥a h·ªçc",
                            "content": f"N·ªôi dung v·ªÅ {prompt}",
                            "duration_minutes": 20,
                            "order": 1
                        }
                    ]
                }
            ]
        }
    except Exception as e:
        print(f"‚ùå Unexpected error in course generation: {type(e).__name__}: {str(e)}")
        import traceback
        print(f"üìç Traceback: {traceback.format_exc()}")
        print(f"üîÑ Returning fallback structure...")
        # Same fallback structure v·ªõi learning_outcomes ƒë√∫ng format
        return {
            "title": f"Kh√≥a h·ªçc v·ªÅ {prompt[:50]}",
            "description": f"Kh√≥a h·ªçc ƒë∆∞·ª£c t·∫°o t·ª± ƒë·ªông d·ª±a tr√™n y√™u c·∫ßu: {prompt}",
            "category": "General",
            "level": difficulty,
            "learning_outcomes": [
                {
                    "id": "lo1",
                    "description": f"Hi·ªÉu ƒë∆∞·ª£c kh√°i ni·ªám c∆° b·∫£n v·ªÅ {prompt[:30]}",
                    "skill_tag": "basic-understanding"
                },
                {
                    "id": "lo2",
                    "description": "√Åp d·ª•ng ƒë∆∞·ª£c ki·∫øn th·ª©c v√†o th·ª±c t·∫ø",
                    "skill_tag": "practical-application"
                }
            ],
            "modules": [
                {
                    "title": "Module 1: Gi·ªõi thi·ªáu",
                    "description": "Module gi·ªõi thi·ªáu kh√≥a h·ªçc",
                    "order": 1,
                    "difficulty": "Basic",
                    "estimated_hours": 2,
                    "learning_outcomes": [
                        {
                            "description": "N·∫Øm ƒë∆∞·ª£c t·ªïng quan v·ªÅ kh√≥a h·ªçc",
                            "skill_tag": "overview"
                        },
                        {
                            "description": "Chu·∫©n b·ªã ki·∫øn th·ª©c n·ªÅn t·∫£ng",
                            "skill_tag": "foundation"
                        }
                    ],
                    "lessons": [
                        {
                            "title": "B√†i 1: T·ªïng quan",
                            "description": "Gi·ªõi thi·ªáu t·ªïng quan v·ªÅ kh√≥a h·ªçc",
                            "content": f"N·ªôi dung v·ªÅ {prompt}",
                            "duration_minutes": 20,
                            "order": 1
                        }
                    ]
                }
            ]
        }


async def generate_module_quiz(
    module_title: str,
    learning_outcomes: List[Dict],
    module_description: Optional[str] = None,
    question_count: int = 10,
    difficulty: str = "medium",
    focus_outcomes: Optional[List[str]] = None
) -> Dict:
    """
    Generate quiz questions from module learning outcomes using Gemini AI.
    
    Args:
        module_title: Title of the module
        learning_outcomes: List of learning outcomes with structure:
            [{"id": str, "outcome": str, "skill_tag": str, "is_mandatory": bool}]
        module_description: Optional description of the module for context
        question_count: Number of questions to generate (5-30)
        difficulty: Difficulty level (easy|medium|hard)
        focus_outcomes: Optional list of outcome IDs to focus on
        
    Returns:
        Dict with structure matching Quiz model schema:
        {
            "questions": [
                {
                    "question": str,
                    "type": "multiple_choice" | "fill_in_blank" | "true_false",
                    "options": List[str],  # Empty for fill_in_blank
                    "correct_answer": str,
                    "explanation": str,
                    "points": int,
                    "is_mandatory": bool,
                    "order": int,
                    "outcome_id": str  # Link to learning outcome
                }
            ],
            "total_points": int,
            "mandatory_count": int,
            "estimated_time_minutes": int
        }
    """
    try:
        # Validate inputs
        if not learning_outcomes or len(learning_outcomes) == 0:
            raise ValueError("Learning outcomes cannot be empty")
        
        if question_count < 5 or question_count > 30:
            raise ValueError("Question count must be between 5 and 30")
        
        if difficulty not in ["easy", "medium", "hard"]:
            difficulty = "medium"
        
        # Limit outcomes to avoid token overflow (max 10)
        # If focus_outcomes specified, prioritize those
        if focus_outcomes and len(focus_outcomes) > 0:
            # Filter to focus outcomes
            focused = [o for o in learning_outcomes if o.get("id") in focus_outcomes]
            if focused:
                outcomes_subset = focused[:10]
            else:
                outcomes_subset = learning_outcomes[:10]
        else:
            outcomes_subset = learning_outcomes[:10]
        
        # Build outcomes context
        outcomes_text = "\n".join([
            f"{i+1}. {outcome.get('outcome', '')} (ID: {outcome.get('id', '')}, B·∫Øt bu·ªôc: {outcome.get('is_mandatory', False)})"
            for i, outcome in enumerate(outcomes_subset)
        ])
        
        # Add module description if available
        context_text = ""
        if module_description:
            context_text = f"\nM√¥ t·∫£ module: {module_description}\n"
        
        # Build prompt with strict JSON structure matching Quiz schema
        prompt = f"""B·∫°n l√† chuy√™n gia gi√°o d·ª•c, h√£y t·∫°o {question_count} c√¢u h·ªèi tr·∫Øc nghi·ªám cho module "{module_title}".

TH√îNG TIN MODULE:
{outcomes_text}
{context_text}

Y√äU C·∫¶U:
1. T·∫°o ch√≠nh x√°c {question_count} c√¢u h·ªèi
2. ƒê·ªô kh√≥: {difficulty}
3. Ph√¢n b·ªï ƒë·ªÅu c√°c lo·∫°i c√¢u h·ªèi:
   - 60% multiple_choice (4 l·ª±a ch·ªçn)
   - 25% fill_in_blank (c√¢u tr·∫£ l·ªùi ng·∫Øn)
   - 15% true_false (ƒë√∫ng/sai)
4. C√¢u h·ªèi cho outcome b·∫Øt bu·ªôc (is_mandatory=True) ph·∫£i ƒë√°nh d·∫•u is_mandatory=True
5. ƒêi·ªÉm m·ªói c√¢u: 5-15 points t√πy ƒë·ªô kh√≥
6. Ph·∫£i c√≥ explanation chi ti·∫øt cho m·ªói c√¢u

ƒê·ªäNH D·∫†NG JSON (QUAN TR·ªåNG - PH·∫¢I TU√ÇN TH·ª¶):
{{
  "questions": [
    {{
      "question": "N·ªôi dung c√¢u h·ªèi chi ti·∫øt",
      "type": "multiple_choice",
      "options": ["A. ƒê√°p √°n 1", "B. ƒê√°p √°n 2", "C. ƒê√°p √°n 3", "D. ƒê√°p √°n 4"],
      "correct_answer": "A. ƒê√°p √°n 1",
      "explanation": "Gi·∫£i th√≠ch chi ti·∫øt t·∫°i sao ƒë√¢y l√† ƒë√°p √°n ƒë√∫ng",
      "points": 10,
      "is_mandatory": false,
      "order": 1
    }}
  ]
}}

CH√ö √ù:
- V·ªõi c√¢u fill_in_blank: options = []
- V·ªõi c√¢u true_false: options = ["ƒê√∫ng", "Sai"]
- correct_answer ph·∫£i kh·ªõp ch√≠nh x√°c v·ªõi 1 trong c√°c options
- order b·∫Øt ƒë·∫ßu t·ª´ 1 v√† tƒÉng d·∫ßn

Ch·ªâ tr·∫£ v·ªÅ JSON thu·∫ßn t√∫y, kh√¥ng c√≥ markdown code block hay text th·ª´a."""

        # Call Gemini AI (use model from settings, not hardcoded)
        quiz_model = genai.GenerativeModel(settings.gemini_model)
        response = quiz_model.generate_content(prompt)
        
        if not response or not response.text:
            raise ValueError("AI did not return any response")
        
        # Extract JSON from response
        response_text = response.text.strip()
        
        # Remove markdown code blocks if present
        if response_text.startswith("```json"):
            response_text = response_text[7:]
        if response_text.startswith("```"):
            response_text = response_text[3:]
        if response_text.endswith("```"):
            response_text = response_text[:-3]
        
        response_text = response_text.strip()
        
        # Parse JSON
        quiz_data = json.loads(response_text)
        
        # Validate response structure
        if "questions" not in quiz_data or not isinstance(quiz_data["questions"], list):
            raise ValueError("Invalid AI response: missing 'questions' array")
        
        questions = quiz_data["questions"]
        
        if len(questions) == 0:
            raise ValueError("AI generated 0 questions")
        
        # Validate and fix each question
        validated_questions = []
        total_points = 0
        mandatory_count = 0
        
        for idx, q in enumerate(questions):
            # Ensure required fields
            if "question" not in q or not q["question"]:
                continue
            
            # Default type
            if "type" not in q or q["type"] not in ["multiple_choice", "fill_in_blank", "true_false"]:
                q["type"] = "multiple_choice"
            
            # Validate options based on type
            if q["type"] == "fill_in_blank":
                q["options"] = []
            elif q["type"] == "true_false":
                q["options"] = ["ƒê√∫ng", "Sai"]
            else:  # multiple_choice
                if "options" not in q or not isinstance(q["options"], list) or len(q["options"]) < 2:
                    continue  # Skip invalid question
            
            # Validate correct_answer
            if "correct_answer" not in q or not q["correct_answer"]:
                continue
            
            # For multiple choice and true/false, correct_answer must be in options
            if q["type"] in ["multiple_choice", "true_false"]:
                if q["correct_answer"] not in q["options"]:
                    # Try to match
                    matched = False
                    for opt in q["options"]:
                        if q["correct_answer"].lower() in opt.lower() or opt.lower() in q["correct_answer"].lower():
                            q["correct_answer"] = opt
                            matched = True
                            break
                    if not matched:
                        continue  # Skip invalid question
            
            # Default explanation
            if "explanation" not in q or not q["explanation"]:
                q["explanation"] = f"ƒê√°p √°n ƒë√∫ng l√†: {q['correct_answer']}"
            
            # Default points (5-15 based on difficulty)
            if "points" not in q or not isinstance(q["points"], int) or q["points"] < 1:
                if difficulty == "easy":
                    q["points"] = 5
                elif difficulty == "hard":
                    q["points"] = 15
                else:
                    q["points"] = 10
            
            # is_mandatory
            if "is_mandatory" not in q:
                q["is_mandatory"] = False
            
            if q["is_mandatory"]:
                mandatory_count += 1
            
            # order
            q["order"] = idx + 1
            
            # Map field names to Quiz model schema
            # AI uses "question", Quiz model uses "question_text"
            if "question" in q and "question_text" not in q:
                q["question_text"] = q["question"]
                del q["question"]
            
            total_points += q["points"]
            validated_questions.append(q)
        
        # Ensure we have at least 3 questions
        if len(validated_questions) < 3:
            raise ValueError(f"AI generated only {len(validated_questions)} valid questions, minimum is 3")
        
        # Estimate time: 1-2 minutes per question
        estimated_time = len(validated_questions) * 2
        
        return {
            "questions": validated_questions,
            "total_points": total_points,
            "mandatory_count": mandatory_count,
            "estimated_time_minutes": estimated_time
        }
    
    except json.JSONDecodeError as e:
        print(f"Failed to parse AI response as JSON: {str(e)}")
        print(f"Response text: {response_text[:500]}")
        print(f"Returning fallback questions for module: {module_title}")
        return _generate_module_quiz_fallback(
            module_title=module_title,
            learning_outcomes=outcomes_subset,
            question_count=question_count,
            difficulty=difficulty
        )
    
    except Exception as e:
        print(f"Error in generate_module_quiz: {str(e)}")
        print(f"Returning fallback questions for module: {module_title}")
        return _generate_module_quiz_fallback(
            module_title=module_title,
            learning_outcomes=outcomes_subset,
            question_count=question_count,
            difficulty=difficulty
        )


def _generate_module_quiz_fallback(
    module_title: str,
    learning_outcomes: List[Dict],
    question_count: int,
    difficulty: str
) -> Dict:
    """
    Generate fallback quiz questions when AI is unavailable.
    
    Args:
        module_title: Title of the module
        learning_outcomes: List of learning outcomes
        question_count: Number of questions to generate
        difficulty: Difficulty level
        
    Returns:
        Dict with quiz structure matching Quiz model schema
    """
    questions = []
    total_points = 0
    mandatory_count = 0
    
    # Calculate distribution based on question count
    mc_count = int(question_count * 0.6)  # 60% multiple choice
    fb_count = int(question_count * 0.25)  # 25% fill in blank
    tf_count = question_count - mc_count - fb_count  # remaining true/false
    
    # Ensure at least 1 of each type if possible
    if question_count >= 3:
        if mc_count == 0:
            mc_count = 1
        if fb_count == 0 and question_count > 3:
            fb_count = 1
        if tf_count == 0 and question_count > 4:
            tf_count = 1
        # Recalculate to ensure sum equals question_count
        total = mc_count + fb_count + tf_count
        if total > question_count:
            tf_count = question_count - mc_count - fb_count
    
    # Set points based on difficulty
    if difficulty == "easy":
        base_points = 5
    elif difficulty == "hard":
        base_points = 15
    else:
        base_points = 10
    
    order = 1
    
    # Generate multiple choice questions
    for i in range(mc_count):
        # Cycle through outcomes
        outcome = learning_outcomes[i % len(learning_outcomes)]
        outcome_text = outcome.get("outcome", "knowledge")
        outcome_id = outcome.get("id", "")
        is_mandatory = outcome.get("is_mandatory", False)
        
        question = {
            "question_text": f"[Fallback] C√¢u h·ªèi tr·∫Øc nghi·ªám v·ªÅ: {outcome_text}",
            "type": "multiple_choice",
            "options": [
                "A. ƒê√°p √°n m·∫´u A",
                "B. ƒê√°p √°n m·∫´u B", 
                "C. ƒê√°p √°n m·∫´u C",
                "D. ƒê√°p √°n m·∫´u D"
            ],
            "correct_answer": "A. ƒê√°p √°n m·∫´u A",
            "explanation": f"Gi·∫£i th√≠ch m·∫´u cho c√¢u h·ªèi v·ªÅ {outcome_text}",
            "points": base_points,
            "is_mandatory": is_mandatory,
            "order": order,
            "outcome_id": outcome_id
        }
        
        if is_mandatory:
            mandatory_count += 1
        
        total_points += base_points
        questions.append(question)
        order += 1
    
    # Generate fill in blank questions
    for i in range(fb_count):
        outcome = learning_outcomes[(mc_count + i) % len(learning_outcomes)]
        outcome_text = outcome.get("outcome", "knowledge")
        outcome_id = outcome.get("id", "")
        is_mandatory = outcome.get("is_mandatory", False)
        
        question = {
            "question_text": f"[Fallback] ƒêi·ªÅn v√†o ch·ªó tr·ªëng li√™n quan ƒë·∫øn: {outcome_text}",
            "type": "fill_in_blank",
            "options": [],
            "correct_answer": "ƒë√°p √°n m·∫´u",
            "explanation": f"Gi·∫£i th√≠ch m·∫´u cho c√¢u ƒëi·ªÅn ch·ªó tr·ªëng v·ªÅ {outcome_text}",
            "points": base_points,
            "is_mandatory": is_mandatory,
            "order": order,
            "outcome_id": outcome_id
        }
        
        if is_mandatory:
            mandatory_count += 1
        
        total_points += base_points
        questions.append(question)
        order += 1
    
    # Generate true/false questions
    for i in range(tf_count):
        outcome = learning_outcomes[(mc_count + fb_count + i) % len(learning_outcomes)]
        outcome_text = outcome.get("outcome", "knowledge")
        outcome_id = outcome.get("id", "")
        is_mandatory = outcome.get("is_mandatory", False)
        
        question = {
            "question_text": f"[Fallback] ƒê√∫ng hay sai: {outcome_text}",
            "type": "true_false",
            "options": ["ƒê√∫ng", "Sai"],
            "correct_answer": "ƒê√∫ng",
            "explanation": f"Gi·∫£i th√≠ch m·∫´u cho c√¢u ƒë√∫ng/sai v·ªÅ {outcome_text}",
            "points": base_points,
            "is_mandatory": is_mandatory,
            "order": order,
            "outcome_id": outcome_id
        }
        
        if is_mandatory:
            mandatory_count += 1
        
        total_points += base_points
        questions.append(question)
        order += 1
    
    # Estimate time: 1.5 minutes per question
    estimated_time = int(question_count * 1.5)
    
    return {
        "questions": questions,
        "total_points": total_points,
        "mandatory_count": mandatory_count,
        "estimated_time_minutes": estimated_time
    }


async def generate_practice_exercises(
    topic: str,
    difficulty: str = "medium",
    question_count: int = 5,
    practice_type: str = "multiple_choice",
    focus_skills: Optional[List[str]] = None,
    learning_outcomes: Optional[List[Dict]] = None,
    content_summary: str = ""
) -> Dict:
    """
    Sinh b√†i t·∫≠p luy·ªán t·∫≠p b·∫±ng AI v·ªõi context ƒë·∫ßy ƒë·ªß
    
    Args:
        topic: Ch·ªß ƒë·ªÅ c·∫ßn luy·ªán t·∫≠p
        difficulty: easy|medium|hard
        question_count: S·ªë c√¢u h·ªèi (1-20)
        practice_type: multiple_choice|short_answer|mixed
        focus_skills: Danh s√°ch skill tags c·∫ßn t·∫≠p trung (optional)
        learning_outcomes: Learning outcomes t·ª´ course/lesson (optional)
        content_summary: T√≥m t·∫Øt n·ªôi dung t·ª´ course/lesson (optional)
        
    Returns:
        Dict with exercises array
    """
    try:
        import sys
        print(f"\nü§ñ AI Service: generate_practice_exercises", flush=True)
        print(f"  Topic: {topic}", flush=True)
        print(f"  Difficulty: {difficulty}", flush=True)
        print(f"  Count: {question_count}", flush=True)
        print(f"  Has learning outcomes: {len(learning_outcomes) if learning_outcomes else 0}", flush=True)
        print(f"  Has content summary: {len(content_summary)}", flush=True)
        
        # Map difficulty
        diff_map = {
            "easy": "d·ªÖ",
            "medium": "trung b√¨nh",
            "hard": "kh√≥"
        }
        diff_vn = diff_map.get(difficulty, "trung b√¨nh")
        
        # Build prompt with context (gi·∫£m context ƒë·ªÉ tr√°nh qu√° d√†i)
        prompt = f"""B·∫°n l√† chuy√™n gia gi√°o d·ª•c, h√£y t·∫°o {question_count} c√¢u h·ªèi luy·ªán t·∫≠p ch·∫•t l∆∞·ª£ng cao.

CH·ª¶ ƒê·ªÄ: {topic}

"""
        
        # Add content summary if available (gi·ªõi h·∫°n 200 chars)
        if content_summary:
            summary_short = content_summary[:200]
            prompt += f"""N·ªòI DUNG THAM KH·∫¢O:
{summary_short}

"""
        
        # Add learning outcomes if available (max 3 outcomes)
        if learning_outcomes and len(learning_outcomes) > 0:
            prompt += "M·ª§C TI√äU H·ªåC T·∫¨P:\n"
            for i, outcome in enumerate(learning_outcomes[:3], 1):  # Max 3 outcomes
                desc = outcome.get("description", "")
                skill = outcome.get("skill_tag", "")
                if desc:
                    # Gi·ªõi h·∫°n description 100 chars
                    desc_short = desc[:100]
                    prompt += f"{i}. {desc_short}"
                    if skill:
                        prompt += f" (K·ªπ nƒÉng: {skill})"
                    prompt += "\n"
            prompt += "\n"
        
        # Add requirements
        prompt += f"""Y√äU C·∫¶U:
1. ƒê·ªô kh√≥: {diff_vn}
2. S·ªë c√¢u h·ªèi: {question_count}
3. Lo·∫°i c√¢u h·ªèi: {practice_type}
"""
        
        if focus_skills:
            prompt += f"4. T·∫≠p trung v√†o c√°c k·ªπ nƒÉng: {', '.join(focus_skills[:3])}\n"  # Max 3 skills
        
        prompt += """
QUAN TR·ªåNG - ƒê·ªäNH D·∫†NG JSON:
Ch·ªâ tr·∫£ v·ªÅ JSON thu·∫ßn t√∫y, KH√îNG c√≥ markdown, KH√îNG c√≥ code block, KH√îNG c√≥ text th·ª´a.

{
  "exercises": [
    {
      "id": "uuid-here",
      "type": "theory",
      "question": "N·ªôi dung c√¢u h·ªèi",
      "options": ["A. ƒê√°p √°n 1", "B. ƒê√°p √°n 2", "C. ƒê√°p √°n 3", "D. ƒê√°p √°n 4"],
      "correct_answer": "A. ƒê√°p √°n 1",
      "explanation": "Gi·∫£i th√≠ch ng·∫Øn g·ªçn",
      "difficulty": "Medium",
      "related_skill": "skill-tag",
      "points": 10
    }
  ]
}

L∆ØU √ù:
- CH·ªà JSON, KH√îNG c√≥ ```json ho·∫∑c ``` hay markdown
- V·ªõi short_answer: options = null
- type: theory, coding, ho·∫∑c problem-solving
- C√¢u h·ªèi ng·∫Øn g·ªçn, r√µ r√†ng
"""

        print(f"  Calling Gemini API...", flush=True)
        
        # Call Gemini AI
        model = genai.GenerativeModel(settings.gemini_model)
        response = model.generate_content(prompt)
        
        response_text = response.text.strip()
        
        print(f"  Response length: {len(response_text)} chars", flush=True)
        print(f"  Response preview: {response_text[:200]}...", flush=True)
        
        # Extract JSON - IMPROVED LOGIC
        json_str = None
        
        # Method 1: Check for ```json specifically
        if "```json" in response_text:
            print(f"  Found ```json block", flush=True)
            json_start = response_text.find("```json") + 7
            json_end = response_text.find("```", json_start)
            if json_end > json_start:
                json_str = response_text[json_start:json_end].strip()
        
        # Method 2: Look for JSON object directly (starts with {)
        if not json_str:
            # Find first { and last }
            first_brace = response_text.find("{")
            last_brace = response_text.rfind("}")
            
            if first_brace != -1 and last_brace != -1 and last_brace > first_brace:
                print(f"  Found JSON object at position {first_brace}-{last_brace}", flush=True)
                json_str = response_text[first_brace:last_brace+1]
            else:
                print(f"  No JSON object found in response", flush=True)
                raise ValueError("No valid JSON found in AI response")
        
        print(f"  Extracted JSON length: {len(json_str)} chars", flush=True)
        print(f"  JSON preview: {json_str[:300]}...", flush=True)
        
        # Parse JSON
        print(f"  Parsing JSON...", flush=True)
        data = json.loads(json_str)
        
        # Validate
        if "exercises" not in data or not isinstance(data["exercises"], list):
            raise ValueError("Invalid response structure - missing exercises array")
        
        print(f"  ‚úÖ Parsed successfully! Found {len(data['exercises'])} exercises", flush=True)
        
        # Add UUIDs if missing
        for ex in data["exercises"]:
            if "id" not in ex or not ex["id"] or ex["id"] == "uuid-here":
                ex["id"] = str(generate_uuid())
        
        return data
        
    except json.JSONDecodeError as e:
        print(f"\n‚ùå JSON Parse Error!", flush=True)
        print(f"  Error: {str(e)}", flush=True)
        print(f"  Position: line {e.lineno}, col {e.colno}", flush=True)
        if 'json_str' in locals():
            print(f"  Problematic JSON:", flush=True)
            lines = json_str.split('\n')
            for i, line in enumerate(lines[:20], 1):  # Show first 20 lines
                print(f"    {i}: {line}", flush=True)
        # Fallback: Return empty
        return {"exercises": []}
        
    except Exception as e:
        print(f"\n‚ùå AI generate_practice_exercises failed!", flush=True)
        print(f"  Error type: {type(e).__name__}", flush=True)
        print(f"  Error: {str(e)}", flush=True)
        import traceback
        traceback.print_exc()
        # Fallback: Return empty
        return {"exercises": []}
